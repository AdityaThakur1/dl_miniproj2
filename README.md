# DL Mini-Project 2: Exploring Self-supervised Learning on Multi-modal Data
*Authors:* Nikunj Gupta, Harish Chauhan, Aditya Thakur 


## Abstract 

The paradigm of supervised learning has proved to perform exceptionally well
on tasks they are trained on using large amount of clean labelled data. However,
there is a limitation with availability of labelled data as it has many associated
costs. In real world, huge amount of raw and unlabeled data is present and with
optimized techniques they can be used to produce meaningful predictions, also
known as self-supervised learning. We have used 4 different pretext tasks (Rota-
tion, SimCLR, Context-Encoder and Contrastive Predictive Coding) to determine
which performs better on CIFAR-10. Furthermore, we live in a world involving
multiple modalities â€“ vision, sound, language, taste, touch, and so on. Artificial
Intelligence (AI) needs to be capable of interpreting and reasoning about multi-
modal inputs to make significant progress in understanding the world around us.
In this project we aim to combine the aforementioned motivations and develop
neural network architectures that learn visual representations via self-supervision
by making use of images and text descriptions widely available on the internet.
In particular we use the Wikipedia multimodal dataset [ 29], implement the CLIP
[28 ] architecture, and perform some ablation studies. 
